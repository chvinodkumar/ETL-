
###### Sample Config #######
"""
{
        "config_path":"redshift connection path",
        "connection_profile":"redshift connection profile",
        "utils_path":"utility py file,
        "environment":"innovation / Prod ",
        "source":"source name",
        "table_name":"contentdocumentlink",
        "soql_query":"SOQL Query from which data has to fetched",
        "instance":"source instance to be connected",
        "schema_name":"target schema name",
        "pagination":"If pagination is required or not as Y/N ",
        "incremental_column":"Audit date column available in source used for incremental load",
        "log_path":"Log folder path without file name"

}

"""

#### Importing Necessary Packages ####
from simple_salesforce import Salesforce
import requests,argparse,json
import pandas as pd
from datetime import datetime
from sqlalchemy import types
import urllib3,os,sys
from io import BytesIO
import boto3
import pyarrow as pa
import pyarrow.parquet as pq

class DataFetcher():
    # A class to fetch data using SOQL Query from Saas based source table and ingest it to Redshift with collation enabled and incremental logic
    def __init__(self,config,saas,logger,engine) -> None:
        """
        The Constructor for DataFetcher class.

        Parameters:
        config (str): Path to config file
        logger (str): Path to log file that will be generated
        engine (object): Redshift connection
        """
        self.config=config
        self.saas=saas
        self.logger=logger
        self.engine=engine
        self.con=self.engine.connect().execution_options(autocommit=True)

    @staticmethod
    def sqlcol(dfparam,lengths):
        """
        A static method to enable collation for varchar data fields

        Parameters:
        dfparam (DataFrame) : DataFrame to which collation has to be enabled
        lengths (dict)      : Fieldname,Data length as key:value pair obtained from source for the respective object

        Returns:
        dtypedict (dict)    : Data types of the DataFrame and adding collation condition for varchar fields in the DataFrame
        """
        dtypedict = {}
        for i, j in zip(dfparam.columns, dfparam.dtypes):
            if "object" in str(j):
                dtypedict.update({i: types.VARCHAR(int(lengths[i]), collation='case_insensitive')})
        logger.info("Collation enabled")
        return dtypedict

    def authentication(self,auth_url,source):
        """
        A method to connect to source and generate token to fetch object data

        Parameters:
        auth_url (str) : Api url which has to be hit to get the necessary response
        source (str)   : Source name

        Returns:
        session_id (str)   : Token received from api response
        instance_url (str) : Api url to hit source to get data for object
        """
        autherization = requests.post(auth_url)
        session_id = autherization.json()['access_token']
        instance_url = autherization.json()['instance_url']
        self.logger.info(f"Generated token to Connect Source - {source}")
        return session_id,instance_url

    def paginate_data_from_saas(self):
        """
        A method to perform paginate the response given by Source to fetch all the records in all pages without data loss

        This method is yet to be created by considering future requirement this will be added
        Parameter:None

        Returns:None
        """
        self.logger.info("Paginate_function to be added")

    def get_data_from_saas(self,session_id,instance_url,loadtype,source):
        """
        A method to connect to Salesforce API and fetch object's fieldnames ,datatypes as one response converted into a dictonary and dataset of the object depending on the loadtype being performed as another reponse which is converted into DataFrame

        Parameters:
        session_id (str)   : Token generated by authentication to connect with instace_url
        instance_url (str) : API url of the source post authentication through which Salesforce connection can be established to access objects and their metadata
        loadtype (str)     : Type of load to be performed for the ingestion
        source (str)       : Source we are trying to connect to fetch response

        Returns:
        fields (dict)      : Fieldname,DataType as key:value pair obtained from source for the respective object
        df(DataFrame)      : Response of the object conneted to Source that has been converted to DataFrame
        lengths (dict)     : Fieldname,Data length as key:value pair obtained from source for the respective object
        """
        self.logger.info(f"Connecting to Source {source}")
        sf = Salesforce(instance_url=instance_url,session_id=session_id)
        fields={}
        lengths={}
        meta=sf.__getattr__(self.config['table_name']).describe()
        for data in meta['fields']:
            fields[data['name']]=data['type']
            lengths[str(data['name']).lower()]=data['length']
        self.logger.info(f"Gathered Fields and it's data types from {source}")
        if loadtype=='incremental':
            self.logger.info(f"Proceeding with {loadtype}")
            query=self.engine.execute(f"select max({self.config['incremental_column']}) from {self.config['schema_name']}.{self.config['table_name']}")
            lastdate=query.scalar()
            query_result = sf.query_all(f"{self.config['soql_query']} and {self.config['incremental_column']} > {str(lastdate).replace(' ','T')}")
        else:
            self.logger.info(f"Proceeding with {loadtype}")
            query_result = sf.query_all(self.config["soql_query"])
        for records in query_result['records']:
            records.pop('attributes')
        df=pd.json_normalize(query_result['records'])
        return df,fields,lengths

    def transformation(self, fields, dataframe,lengths):
        """
        A method to compare data types with source and do any conversions in the DataFrame if required and convert the DataFrame to Parquet file. If table is new, it will also create necessary tables to proceed with ingestion.

        Parameters:
        fields (dict)          : Fieldname, DataType as key:value pair obtained from source for the respective object
        dataframe (DataFrame)  : DataFrame of the response data given by API call for the required object
        lengths (dict)         : Fieldname,Data length as key:value pair obtained from source for the respective object

        Returns:
        parquet_data (parquet) : DataFrame converted into Parquet data post conversions
        """
        self.logger.info("Performing data type conversion")
        for column, dtype in dataframe.dtypes.items():
            field_type = fields.get(column)
            if field_type:
                if field_type == 'int' and not pd.api.types.is_integer_dtype(dtype):
                    dataframe[column] = dataframe[column].astype('int')
                elif field_type in ['double', 'currency', 'percent'] and not pd.api.types.is_float_dtype(dtype):
                    dataframe[column] = dataframe[column].astype('float')
                elif 'date' in field_type and not pd.api.types.is_datetime64_any_dtype(dtype):
                    dataframe[column] = pd.to_datetime(dataframe[column])
                elif 'boolean' == field_type and not pd.api.types.is_bool_dtype(dtype):
                    dataframe[column]=dataframe[column].astype('bool')
        dataframe["timestamp"]=pd.to_datetime(datetime.now())
        dataframe.columns=pd.Series(dataframe.columns).str.lower()
        self.logger.info("Converting DataFrame to Parquet file")
        table = pa.Table.from_pandas(dataframe,preserve_index=False)
        parquet_data = BytesIO()
        pq.write_table(table, parquet_data)
        parquet_data.seek(0)
        query = self.engine.execute(f"""SELECT EXISTS (SELECT * FROM information_schema.tables WHERE table_schema='{self.config["schema_name"]}' and table_name='{self.config["table_name"]}_stg')""")
        result = query.scalar()
        if result==False:
            dataframe.head(0).to_sql(name=f"{self.config['table_name']}",schema=self.config['schema_name'],if_exists='replace',index=False,con=self.con,dtype=self.sqlcol(dataframe,lengths))
            self.logger.info(f"{self.config['schema_name']}.{self.config['table_name']} has been created")
            dataframe.head(0).to_sql(name=f"{self.config['table_name']}_stg",schema=self.config['schema_name'],if_exists='replace',index=False,con=self.con,dtype=self.sqlcol(dataframe,lengths))
            self.logger.info(f"{self.config['schema_name']}.{self.config['table_name']}_stg has been created")
        return parquet_data

    def parq_to_s3(self,parquet_data):
        """
        A method to place the parquet file in S3

        Parameters:
        parquet_data (str) : Token generated by authentication to connect with instace_url

        Returns:
        bucket(str)  :
        key (str)    : Response of the object conneted to Source that has been converted to DataFrame
        """
        session = boto3.Session(profile_name=self.config["environment"])
        s3_client = session.client('s3')
        self.logger.info(f"Connection successfull to {self.config['environment']} S3")
        path = self.saas["s3_path"].get(self.config["environment"])[0]
        bucket=path.split('/')[0]
        if self.config["source"].lower() in self.config["schema_name"].lower():
            key=f"{path.split('/')[1]}/{self.config['source']}/{self.config['table_name']}/year={year}/month={month}/day={day}/{self.config['table_name']}_{start}"
        else:
            source=self.config["schema_name"].split("_")[0]
            key=f"{path.split('/')[1]}/{source}/{self.config['table_name']}/year={year}/month={month}/day={day}/{self.config['table_name']}_{start}"
        s3_client.put_object(Bucket=bucket, Key=key, Body=parquet_data.getvalue())
        self.logger.info(f"{key} file had been created in {bucket}")
        return bucket,key

    def copy_redshift(self,bucket,key,loadtype):
        """
        A method to copy paruet file from S3 to redshift table depending on the loadtype

        Parameters:
        bucket (str)   : S3 bucket name to which parquet file has been ingested
        key (str)      : Key inclusive of prefix and parquet file name
        loadtype (str) : Loadtype incremental/fullload/creation

        Returns:None
        """

        table=f"{self.config['schema_name']}.{self.config['table_name']}"
        if loadtype=='incremental':
            table=table+'_stg'
            copy_query=f"""COPY {table} FROM 's3://{bucket}/{key}' iam_role '{self.saas["s3_path"].get(self.config["environment"])[1]}' FORMAT AS PARQUET;"""
            self.con.execute(copy_query)
            self.logger.info(f"Copy Completed from {bucket}/{key} to {table}")
            self.logger.info(f"Insert completed to {table}")
            self.engine.execute(f""" DELETE FROM {self.config['schema_name']}.{self.config['table_name']} USING {table} WHERE {self.config['schema_name']}.{self.config['table_name']}.id = {table}.id """)
            self.logger.info("Deleted matching records in main table picked up in incremental load from source")
            self.engine.execute(f"""INSERT INTO {self.config['schema_name']}.{self.config['table_name']} SELECT * FROM {table}""")
            self.logger.info(f"Data ingested to {self.config['schema_name']}.{self.config['table_name']} from {table}")
        else:
            copy_query=f"""COPY {table} FROM 's3://{bucket}/{key}' iam_role '{self.saas["s3_path"].get(self.config["environment"])[1]}' FORMAT AS PARQUET;"""
            self.con.execute(copy_query)
            self.logger.info(f"Copy completed from {bucket}/{key} to {table}")
            self.logger.info(f"Data ingested to {table}")

    def main(self):
        """
        A method to check for table existence in Redshift and decide on what loadtype to be executed and call the necessary function
        """

        self.logger.info("fetching count")
        query = self.engine.execute(f"""SELECT EXISTS (SELECT * FROM information_schema.tables WHERE table_schema='{self.config["schema_name"]}' and table_name='{self.config["table_name"]}')""")
        result = query.scalar()
        loadtype="fullload" if result==False else self.config["load_type"]
        source = 'salesforce' if self.config["source"].lower() == 'apttus' else self.config["source"].lower()
        instance = self.saas[source].get(self.config["instance"])
        session_id, instance_url = self.authentication(instance,source)
        if self.config['pagination']=='N':
            data,fields,lengths = self.get_data_from_saas(session_id=session_id, instance_url=instance_url, loadtype=loadtype,source=source)
        # else:pass
        if data.empty:
            self.logger.info("No incremental data to pull")
        else:
            parq_data = self.transformation(fields=fields, dataframe=data,lengths=lengths)
            Bucket, Key = self.parq_to_s3(parquet_data=parq_data)
            self.copy_redshift(bucket=Bucket, key=Key,loadtype=loadtype)
            self.logger.info("Ingestion Completed")

if __name__=="__main__":
    start=datetime.today().strftime("%Y-%m-%d_%H:%M:%S")
    year,month,day=datetime.today().strftime("%Y"),datetime.today().strftime("%m"),datetime.today().strftime("%d")
    parser = argparse.ArgumentParser()
    parser.add_argument('--infile', nargs=2, help="JSON file to be processed", type=argparse.FileType('r'))
    arguments=parser.parse_args()
    saas_config = json.load(arguments.infile[0])
    ingestion_config = json.load(arguments.infile[1])
    parent_path = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(ingestion_config['utils_path'])
    from utils import setup_logger,send_email_notification,get_connection
    log_filename = str(arguments.infile[1].name).split('/')[-1].replace('json', 'log')
    logger = setup_logger(os.path.join(ingestion_config["log_path"], log_filename))
    logger.info("Ingestion Started")
    logger.info(f"Ingestion Config path - {log_filename.replace('log','json')}")
    try:
        engine = get_connection(ingestion_config["config_path"], ingestion_config["connection_profile"])
        logger.info("Redshift connection established")
        data_fetcher=DataFetcher(ingestion_config,saas_config,logger,engine)
        exit(data_fetcher.main())
    except Exception as e:
        logger.error(f"Exception occured-> {e}")
        send_email_notification(message=f"Exception - {e} occured at {parent_path}", subject=f"{ingestion_config['environment']} {ingestion_config['source']} Ingestion Failure --> {ingestion_config['schema_name']}.{ingestion_config['table_name']} ",log_path=os.path.join(ingestion_config["log_path"], log_filename))





